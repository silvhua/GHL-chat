{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(r\"/home/silvhua/custom_python\")\n",
    "sys.path.append(r'/home/silvhua/repositories/GHL-chat/src/')\n",
    "from silvhua import *\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory.chat_message_histories import DynamoDBChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain.agents import Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation_dict = dict()\n",
    "system_message_dict = dict()\n",
    "reply_dict = dict()\n",
    "contacts = load_json('contacts.json', '../src/app/private')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history length: 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 120\u001b[0m\n\u001b[1;32m    111\u001b[0m system_message_dict[conversation_id] \u001b[38;5;241m=\u001b[39m create_system_message(\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoachMcloone\u001b[39m\u001b[38;5;124m'\u001b[39m, prompts_filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../prompts\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    113\u001b[0m     examples_filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/chat_examples\u001b[39m\u001b[38;5;124m'\u001b[39m, doc_filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/rag_docs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m conversation_dict[conversation_id] \u001b[38;5;241m=\u001b[39m create_chatbot(\n\u001b[1;32m    116\u001b[0m     contactId, system_message_dict[conversation_id], tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# model='gpt-4-32k'\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 120\u001b[0m reply_dict[conversation_id][question_id] \u001b[38;5;241m=\u001b[39m \u001b[43mchat_with_chatbot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mInboundMessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconversation_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m reply_text \u001b[38;5;241m=\u001b[39m reply_dict[conversation_id][question_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReply from `chat_with_chatbot`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreply_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 85\u001b[0m, in \u001b[0;36mchat_with_chatbot\u001b[0;34m(user_input, agent_info)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChat history length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(agent_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m agent_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 85\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent_executor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain/agents/agent.py:1312\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1312\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1321\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1322\u001b[0m         )\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain/agents/agent.py:1038\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1038\u001b[0m         [\n\u001b[1;32m   1039\u001b[0m             a\n\u001b[1;32m   1040\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1042\u001b[0m                 color_mapping,\n\u001b[1;32m   1043\u001b[0m                 inputs,\n\u001b[1;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1045\u001b[0m                 run_manager,\n\u001b[1;32m   1046\u001b[0m             )\n\u001b[1;32m   1047\u001b[0m         ]\n\u001b[1;32m   1048\u001b[0m     )\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain/agents/agent.py:1038\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1038\u001b[0m         [\n\u001b[1;32m   1039\u001b[0m             a\n\u001b[1;32m   1040\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1042\u001b[0m                 color_mapping,\n\u001b[1;32m   1043\u001b[0m                 inputs,\n\u001b[1;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1045\u001b[0m                 run_manager,\n\u001b[1;32m   1046\u001b[0m             )\n\u001b[1;32m   1047\u001b[0m         ]\n\u001b[1;32m   1048\u001b[0m     )\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain/agents/agent.py:1066\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain/agents/openai_functions_agent/base.py:95\u001b[0m, in \u001b[0;36mOpenAIFunctionsAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, with_functions, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m messages \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_functions:\n\u001b[0;32m---> 95\u001b[0m     predicted_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     predicted_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mpredict_messages(\n\u001b[1;32m    102\u001b[0m         messages,\n\u001b[1;32m    103\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    104\u001b[0m     )\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain_core/language_models/chat_models.py:685\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    684\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain_core/language_models/chat_models.py:632\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 632\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain_core/language_models/chat_models.py:378\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    377\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 378\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    379\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    380\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    382\u001b[0m ]\n\u001b[1;32m    383\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain_core/language_models/chat_models.py:368\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 368\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m         )\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain_core/language_models/chat_models.py:524\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain_community/chat_models/openai.py:434\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    429\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    433\u001b[0m }\n\u001b[0;32m--> 434\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain_community/chat_models/openai.py:352\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/openai/_utils/_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/openai/_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    938\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}"
     ]
    }
   ],
   "source": [
    "def create_system_message(\n",
    "        business_name, prompts_filepath='../prompts',\n",
    "        examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    "        ):\n",
    "    \n",
    "    instructions = load_txt(f'{business_name}.md', prompts_filepath)\n",
    "    examples = load_txt(f'{business_name}.txt', examples_filepath)\n",
    "    document = load_txt(f'{business_name}_doc.md', doc_filepath)\n",
    "\n",
    "    system_message = f\"\"\"# Stage 1\n",
    "\n",
    "{instructions}\n",
    "\n",
    "## Examples\n",
    "\n",
    "{examples}\n",
    "\n",
    "## Relevant documentation\n",
    "\n",
    "{document}\n",
    "\n",
    "# Stage 2\n",
    "\n",
    "Review your response from stage 1. \n",
    "Revise your response if needed to make sure you followed the instructions.\n",
    "Make sure that if the question cannot be answered through the documentation, \n",
    "you return \"[ALERT HUMAN]\".\n",
    "\n",
    "# Stage 3\n",
    "\n",
    "Review your response from stage 2 to ensure your response is concise.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Write the next OutboundMessage based on the following InboundMessage, \n",
    "    which is delimited by triple backticks: ```{InboundMessage}```\n",
    "    \"\"\"\n",
    "    system_message = f'{system_message}{prompt}'\n",
    "    return system_message\n",
    "\n",
    "def create_chatbot(contactId, system_message, tools, model=\"gpt-3.5-turbo-1106\", verbose=True):\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        temperature = 0,\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key'],\n",
    "        model=model, \n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}} # https://platform.openai.com/docs/guides/text-generation/json-mode  # https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.openai.ChatOpenAI.html?highlight=chatopenai#\n",
    "        )\n",
    "    message_history = DynamoDBChatMessageHistory(\n",
    "        table_name=\"SessionTable\", session_id=contactId,\n",
    "        key={\n",
    "            \"SessionId\": contactId,\n",
    "            \"type\": 'ChatHistory',\n",
    "            }\n",
    "        )\n",
    "    system_message = SystemMessage(\n",
    "        content=(system_message),\n",
    "        input_variables=['InboundMessage']\n",
    "    )\n",
    "    \n",
    "    prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "        system_message=system_message,\n",
    "        extra_prompt_messages=[\n",
    "            MessagesPlaceholder(variable_name='chat_history')\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, tools=tools, \n",
    "        verbose=verbose, return_intermediate_steps=True\n",
    "        )\n",
    "    agent_info = {\n",
    "        'agent': agent,\n",
    "        'agent_executor': agent_executor,\n",
    "        'chat_history': message_history.messages\n",
    "    }\n",
    "    return agent_info\n",
    "\n",
    "def chat_with_chatbot(user_input, agent_info):\n",
    "    start_time = time()\n",
    "    print(f'Chat history length: {len(agent_info[\"chat_history\"])}')\n",
    "    chat_history = agent_info['chat_history']\n",
    "    result = agent_info['agent_executor']({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": chat_history\n",
    "        })\n",
    "    print(f'Response time: {time() - start_time} seconds')\n",
    "    \n",
    "    return result\n",
    "\n",
    "def fake_func(inp: str) -> str:\n",
    "    return \"foo\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=f\"foo-{i}\",\n",
    "        func=fake_func,\n",
    "        description=f\"a silly function that you can use to get more information about the number {i}\",\n",
    "    )\n",
    "    for i in range(2)\n",
    "]\n",
    "\n",
    "conversation_id = 1\n",
    "InboundMessage = \"Can we book for Tuesday 9am?\"\n",
    "reply_dict[conversation_id] = dict()\n",
    "question_id = 1\n",
    "\n",
    "contactId = contacts['me_mcloone']\n",
    "system_message_dict[conversation_id] = create_system_message(\n",
    "    'CoachMcloone', prompts_filepath='../prompts',\n",
    "    examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    ")\n",
    "conversation_dict[conversation_id] = create_chatbot(\n",
    "    contactId, system_message_dict[conversation_id], tools=tools,\n",
    "    # model='gpt-4-32k'\n",
    "    )\n",
    "\n",
    "reply_dict[conversation_id][question_id] = chat_with_chatbot(\n",
    "    InboundMessage, conversation_dict[conversation_id]\n",
    ")\n",
    "reply_text = reply_dict[conversation_id][question_id][\"output\"]\n",
    "print(f'Reply from `chat_with_chatbot`: {reply_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history length: 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `foo-0` with `How long is the intro session`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mfoo\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"response\": \"[ALERT HUMAN]\",\n",
      "  \"alert_human\": true\n",
      "}\n",
      "\n",
      "  {\"response\": \"[ALERT HUMAN]\", \"alert_human\": true}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response time: 3.3201441764831543 seconds\n",
      "Reply from `chat_with_chatbot`: {\n",
      "  \"response\": \"[ALERT HUMAN]\",\n",
      "  \"alert_human\": true\n",
      "}\n",
      "\n",
      "  {\"response\": \"[ALERT HUMAN]\", \"alert_human\": true}\n"
     ]
    }
   ],
   "source": [
    "def create_system_message(\n",
    "        business_name, prompts_filepath='../prompts',\n",
    "        examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    "        ):\n",
    "    \n",
    "    instructions = load_txt(f'{business_name}.md', prompts_filepath)\n",
    "    examples = load_txt(f'{business_name}.txt', examples_filepath)\n",
    "    document = load_txt(f'{business_name}_doc.md', doc_filepath)\n",
    "\n",
    "    system_message = f\"\"\"# Stage 1\n",
    "\n",
    "{instructions}\n",
    "\n",
    "Return your response on a JSON format with the following keys:\n",
    "- \"response\" (string): The response to the InboundMessage.\n",
    "- \"alert_human\" (True or False): Whether or not to alert a human to review the response.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are example conversations with leads. Each lead as a unique contact ID.\n",
    "An InboundMessage is from the lead. An OutboundMessage is from you.\n",
    "\n",
    "{examples}\n",
    "\n",
    "## Relevant documentation\n",
    "\n",
    "{document}\n",
    "\n",
    "# Stage 2\n",
    "\n",
    "Review your response from stage 1. \n",
    "Revise your response if needed to make sure you followed the instructions.\n",
    "Make sure that if the question cannot be answered through the documentation, \n",
    "you return \"[ALERT HUMAN]\".\n",
    "\n",
    "# Stage 3\n",
    "\n",
    "Review your response from stage 2 to ensure your response is concise.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Write the next OutboundMessage based on the following InboundMessage, \n",
    "    which is delimited by triple backticks: ```{InboundMessage}```\n",
    "    \"\"\"\n",
    "    system_message = f'{system_message}{prompt}'\n",
    "    return system_message\n",
    "\n",
    "def create_chatbot(contactId, system_message, tools, model=\"gpt-3.5-turbo-1106\", verbose=True):\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        temperature = 0,\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key'],\n",
    "        model=model, \n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}} # https://platform.openai.com/docs/guides/text-generation/json-mode  # https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.openai.ChatOpenAI.html?highlight=chatopenai#\n",
    "        )\n",
    "    message_history = DynamoDBChatMessageHistory(\n",
    "        table_name=\"SessionTable\", session_id=contactId,\n",
    "        key={\n",
    "            \"SessionId\": contactId,\n",
    "            \"type\": 'ChatHistory',\n",
    "            }\n",
    "        )\n",
    "    system_message = SystemMessage(\n",
    "        content=(system_message),\n",
    "        input_variables=['InboundMessage']\n",
    "    )\n",
    "    \n",
    "    prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "        system_message=system_message,\n",
    "        extra_prompt_messages=[\n",
    "            MessagesPlaceholder(variable_name='chat_history')\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, tools=tools, \n",
    "        verbose=verbose, return_intermediate_steps=True\n",
    "        )\n",
    "    agent_info = {\n",
    "        'agent': agent,\n",
    "        'agent_executor': agent_executor,\n",
    "        'chat_history': message_history.messages\n",
    "    }\n",
    "    return agent_info\n",
    "\n",
    "def chat_with_chatbot(user_input, agent_info):\n",
    "    start_time = time()\n",
    "    print(f'Chat history length: {len(agent_info[\"chat_history\"])}')\n",
    "    chat_history = agent_info['chat_history']\n",
    "    result = agent_info['agent_executor']({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": chat_history\n",
    "        })\n",
    "    print(f'Response time: {time() - start_time} seconds')\n",
    "    \n",
    "    return result\n",
    "\n",
    "def fake_func(inp: str) -> str:\n",
    "    return \"foo\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=f\"foo-{i}\",\n",
    "        func=fake_func,\n",
    "        description=f\"a silly function that you can use to get more information about the number {i}\",\n",
    "    )\n",
    "    for i in range(2)\n",
    "]\n",
    "\n",
    "conversation_id = 1.1\n",
    "InboundMessage = \"Can we book for Tuesday 9am?\"\n",
    "reply_dict[conversation_id] = dict()\n",
    "question_id = 1\n",
    "\n",
    "contactId = contacts['me_mcloone']\n",
    "system_message_dict[conversation_id] = create_system_message(\n",
    "    'CoachMcloone', prompts_filepath='../prompts',\n",
    "    examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    ")\n",
    "conversation_dict[conversation_id] = create_chatbot(\n",
    "    contactId, system_message_dict[conversation_id], tools=tools,\n",
    "    # model='gpt-4-32k'\n",
    "    )\n",
    "\n",
    "reply_dict[conversation_id][question_id] = chat_with_chatbot(\n",
    "    InboundMessage, conversation_dict[conversation_id]\n",
    ")\n",
    "reply_text = reply_dict[conversation_id][question_id][\"output\"]\n",
    "print(f'Reply from `chat_with_chatbot`: {reply_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history length: 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "{\n",
      "  \"response\": \"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle.\",\n",
      "  \"alert_human\": true\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response time: 2.0846633911132812 seconds\n",
      "Reply from `chat_with_chatbot`: \n",
      "{\n",
      "  \"response\": \"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle.\",\n",
      "  \"alert_human\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "InboundMessage = \"I have cancer. Will you be able treat me?\"\n",
    "reply_dict[conversation_id] = dict()\n",
    "question_id += 1\n",
    "\n",
    "reply_dict[conversation_id][question_id] = chat_with_chatbot(\n",
    "    InboundMessage, conversation_dict[conversation_id]\n",
    ")\n",
    "reply_text = reply_dict[conversation_id][question_id][\"output\"]\n",
    "print(f'Reply from `chat_with_chatbot`: {reply_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history length: 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `foo-0` with `Are you currently on a health and fitness journey or looking to start one?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mfoo\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "    {\n",
      "        \"response\": \"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle.\",\n",
      "        \"alert_human\": false\n",
      "    }\n",
      "\n",
      "    {\n",
      "        \"response\": \"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\n",
      "        \"alert_human\": false\n",
      "    }\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response time: 4.526203632354736 seconds\n",
      "Reply from `chat_with_chatbot`: \n",
      "    {\n",
      "        \"response\": \"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle.\",\n",
      "        \"alert_human\": false\n",
      "    }\n",
      "\n",
      "    {\n",
      "        \"response\": \"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\n",
      "        \"alert_human\": false\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "InboundMessage = \"Can you help me rehabilitate from a herniated disc?\"\n",
    "question_id += 1\n",
    "\n",
    "reply_dict[conversation_id][question_id] = chat_with_chatbot(\n",
    "    InboundMessage, conversation_dict[conversation_id]\n",
    ")\n",
    "reply_text = reply_dict[conversation_id][question_id][\"output\"]\n",
    "print(f'Reply from `chat_with_chatbot`: {reply_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Can you help me rehabilitate from a herniated disc?',\n",
       " 'chat_history': [HumanMessage(content='How long is the intro session'),\n",
       "  AIMessage(content='hey silvia this is jayson testing for brian')],\n",
       " 'output': '\\n    {\\n        \"response\": \"I\\'m Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle.\",\\n        \"alert_human\": false\\n    }\\n\\n    {\\n        \"response\": \"I\\'m Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\\n        \"alert_human\": false\\n    }',\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='foo-0', tool_input='Are you currently on a health and fitness journey or looking to start one?', log='\\nInvoking: `foo-0` with `Are you currently on a health and fitness journey or looking to start one?`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"Are you currently on a health and fitness journey or looking to start one?\"}', 'name': 'foo-0'}})]),\n",
       "   'foo')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_dict[conversation_id][question_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history length: 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `foo-0` with `How much does the program cost?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mfoo\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"response\": \"Ah yes. I say im willing to work within your  budget! Why dont we make sure I am the best fit first? Fair enough? Although my program is affordable I assure you it will definitely be the best investment you make in yourself!\",\n",
      "  \"alert_human\": false\n",
      "}\n",
      "\n",
      "    {\n",
      "        \"response\": \"Ah yes. I say im willing to work within your  budget! Why dont we make sure I am the best fit first? Fair enough? Although my program is affordable I assure you it will definitely be the best investment you make in yourself!\",\n",
      "        \"alert_human\": false\n",
      "    }\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response time: 4.23983359336853 seconds\n",
      "Reply from `chat_with_chatbot`: {\n",
      "  \"response\": \"Ah yes. I say im willing to work within your  budget! Why dont we make sure I am the best fit first? Fair enough? Although my program is affordable I assure you it will definitely be the best investment you make in yourself!\",\n",
      "  \"alert_human\": false\n",
      "}\n",
      "\n",
      "    {\n",
      "        \"response\": \"Ah yes. I say im willing to work within your  budget! Why dont we make sure I am the best fit first? Fair enough? Although my program is affordable I assure you it will definitely be the best investment you make in yourself!\",\n",
      "        \"alert_human\": false\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "InboundMessage = \"What is the cost of the program?\"\n",
    "question_id += 1\n",
    "\n",
    "reply_dict[conversation_id][question_id] = chat_with_chatbot(\n",
    "    InboundMessage, conversation_dict[conversation_id]\n",
    ")\n",
    "reply_text = reply_dict[conversation_id][question_id][\"output\"]\n",
    "print(f'Reply from `chat_with_chatbot`: {reply_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history length: 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `foo-0` with `Can you give me an Olympic lifting program to improve my technique?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mfoo\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"response\": \"[ALERT HUMAN]\",\n",
      "  \"alert_human\": true\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response time: 2.3843140602111816 seconds\n",
      "Reply from `chat_with_chatbot`: {\n",
      "  \"response\": \"[ALERT HUMAN]\",\n",
      "  \"alert_human\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "InboundMessage = \"Can you give me an Olympic lifting program to improve my technique?\"\n",
    "question_id += 1\n",
    "\n",
    "reply_dict[conversation_id][question_id] = chat_with_chatbot(\n",
    "    InboundMessage, conversation_dict[conversation_id]\n",
    ")\n",
    "reply_text = reply_dict[conversation_id][question_id][\"output\"]\n",
    "print(f'Reply from `chat_with_chatbot`: {reply_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `parse_json_string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': '[ALERT HUMAN]', 'alert_human': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def parse_json_string(json_string):\n",
    "    return json.loads(json_string)\n",
    "\n",
    "reply_dict = parse_json_string(reply_text)\n",
    "reply_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_dict['alert_human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history length: 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `foo-0` with `Are you an AI or a human?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mfoo\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"response\": \"Are you an AI or a human?\",\n",
      "  \"alert_human\": true\n",
      "}\n",
      "\n",
      "    {\n",
      "        \"response\": \"Are you an AI or a human?\",\n",
      "        \"alert_human\": true\n",
      "    }\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response time: 2.6719188690185547 seconds\n",
      "Reply from `chat_with_chatbot`: {\n",
      "  \"response\": \"Are you an AI or a human?\",\n",
      "  \"alert_human\": true\n",
      "}\n",
      "\n",
      "    {\n",
      "        \"response\": \"Are you an AI or a human?\",\n",
      "        \"alert_human\": true\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "def create_system_message(\n",
    "        business_name, prompts_filepath='../prompts',\n",
    "        examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    "        ):\n",
    "    \n",
    "    instructions = load_txt(f'{business_name}.md', prompts_filepath)\n",
    "    examples = load_txt(f'{business_name}.txt', examples_filepath)\n",
    "    document = load_txt(f'{business_name}_doc.md', doc_filepath)\n",
    "\n",
    "    system_message = f\"\"\"# Stage 1\n",
    "\n",
    "{instructions}\n",
    "\n",
    "Return your response on a JSON format with the following keys:\n",
    "- \"response\" (string): The response to the InboundMessage.\n",
    "- \"alert_human\" (True or False): Whether or not to alert a human to review the response.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are example conversations with leads. Each lead as a unique contact ID.\n",
    "An InboundMessage is from the lead. An OutboundMessage is from you.\n",
    "\n",
    "{examples}\n",
    "\n",
    "## Relevant documentation\n",
    "\n",
    "{document}\n",
    "\n",
    "# Stage 2\n",
    "\n",
    "Review your response from stage 1. \n",
    "Revise your response if needed to make sure you followed the instructions.\n",
    "Make sure that if the question cannot be answered through the documentation, \n",
    "you return \"[ALERT HUMAN]\".\n",
    "\n",
    "# Stage 3\n",
    "\n",
    "Review your response from stage 2 to ensure your response is concise.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Write the next OutboundMessage based on the following InboundMessage, \n",
    "    which is delimited by triple backticks: ```{InboundMessage}```\n",
    "    \"\"\"\n",
    "    system_message = f'{system_message}{prompt}'\n",
    "    return system_message\n",
    "\n",
    "def create_chatbot(contactId, system_message, tools, model=\"gpt-3.5-turbo-1106\", verbose=True):\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        temperature = 0,\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key'],\n",
    "        model=model, \n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}} # https://platform.openai.com/docs/guides/text-generation/json-mode  # https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.openai.ChatOpenAI.html?highlight=chatopenai#\n",
    "        )\n",
    "    message_history = DynamoDBChatMessageHistory(\n",
    "        table_name=\"SessionTable\", session_id=contactId,\n",
    "        key={\n",
    "            \"SessionId\": contactId,\n",
    "            \"type\": 'ChatHistory',\n",
    "            }\n",
    "        )\n",
    "    system_message = SystemMessage(\n",
    "        content=(system_message),\n",
    "        input_variables=['InboundMessage']\n",
    "    )\n",
    "    \n",
    "    prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "        system_message=system_message,\n",
    "        extra_prompt_messages=[\n",
    "            MessagesPlaceholder(variable_name='chat_history')\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, tools=tools, \n",
    "        verbose=verbose, return_intermediate_steps=True\n",
    "        )\n",
    "    agent_info = {\n",
    "        'agent': agent,\n",
    "        'agent_executor': agent_executor,\n",
    "        'chat_history': message_history.messages\n",
    "    }\n",
    "    return agent_info\n",
    "\n",
    "def chat_with_chatbot(user_input, agent_info):\n",
    "    start_time = time()\n",
    "    print(f'Chat history length: {len(agent_info[\"chat_history\"])}')\n",
    "    chat_history = agent_info['chat_history']\n",
    "    result = agent_info['agent_executor']({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": chat_history\n",
    "        })\n",
    "    print(f'Response time: {time() - start_time} seconds')\n",
    "    \n",
    "    return result\n",
    "\n",
    "def fake_func(inp: str) -> str:\n",
    "    return \"foo\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=f\"foo-{i}\",\n",
    "        func=fake_func,\n",
    "        description=f\"a silly function that you can use to get more information about the number {i}\",\n",
    "    )\n",
    "    for i in range(2)\n",
    "]\n",
    "\n",
    "conversation_id = 2\n",
    "InboundMessage = \"I have a herniated disc. Can you still coach me?\"\n",
    "reply_dict[conversation_id] = dict()\n",
    "question_id = 1\n",
    "\n",
    "contactId = contacts['me_mcloone']\n",
    "system_message_dict[conversation_id] = create_system_message(\n",
    "    'CoachMcloone', prompts_filepath='../prompts',\n",
    "    examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    ")\n",
    "conversation_dict[conversation_id] = create_chatbot(\n",
    "    contactId, system_message_dict[conversation_id], tools=tools,\n",
    "    # model='gpt-4-32k'\n",
    "    )\n",
    "\n",
    "reply_dict[conversation_id][question_id] = chat_with_chatbot(\n",
    "    InboundMessage, conversation_dict[conversation_id]\n",
    ")\n",
    "reply_text = reply_dict[conversation_id][question_id][\"output\"]\n",
    "print(f'Reply from `chat_with_chatbot`: {reply_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 3 update the function tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history length: 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `alert_human_function` with `Can I still coach someone with a herniated disc?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[ALERT HUMAN]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "{\"response\":\"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\"alert_human\":true}\n",
      "\n",
      "{\"response\":\"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\"alert_human\":true}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response time: 4.430384635925293 seconds\n",
      "Reply from `chat_with_chatbot`: \n",
      "{\"response\":\"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\"alert_human\":true}\n",
      "\n",
      "{\"response\":\"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\"alert_human\":true}\n"
     ]
    }
   ],
   "source": [
    "def create_system_message(\n",
    "        business_name, prompts_filepath='../prompts',\n",
    "        examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    "        ):\n",
    "    \n",
    "    instructions = load_txt(f'{business_name}.md', prompts_filepath)\n",
    "    examples = load_txt(f'{business_name}.txt', examples_filepath)\n",
    "    document = load_txt(f'{business_name}_doc.md', doc_filepath)\n",
    "\n",
    "    system_message = f\"\"\"# Stage 1\n",
    "\n",
    "{instructions}\n",
    "\n",
    "Return your response on a JSON format with the following keys:\n",
    "- \"response\" (string): The response to the InboundMessage.\n",
    "- \"alert_human\" (True or False): Whether or not to alert a human to review the response.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are example conversations with leads. Each lead as a unique contact ID.\n",
    "An InboundMessage is from the lead. An OutboundMessage is from you.\n",
    "\n",
    "{examples}\n",
    "\n",
    "## Relevant documentation\n",
    "\n",
    "{document}\n",
    "\n",
    "# Stage 2\n",
    "\n",
    "Review your response from stage 1. \n",
    "Revise your response if needed to make sure you followed the instructions.\n",
    "Make sure that if the question cannot be answered through the documentation, \n",
    "you return \"[ALERT HUMAN]\".\n",
    "\n",
    "# Stage 3\n",
    "\n",
    "Review your response from stage 2 to ensure your response is concise.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Write the next OutboundMessage based on the following InboundMessage, \n",
    "    which is delimited by triple backticks: ```{InboundMessage}```\n",
    "    \"\"\"\n",
    "    system_message = f'{system_message}{prompt}'\n",
    "    return system_message\n",
    "\n",
    "def create_chatbot(contactId, system_message, tools, model=\"gpt-3.5-turbo-1106\", verbose=True):\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        temperature = 0,\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key'],\n",
    "        model=model, \n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}} # https://platform.openai.com/docs/guides/text-generation/json-mode  # https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.openai.ChatOpenAI.html?highlight=chatopenai#\n",
    "        )\n",
    "    message_history = DynamoDBChatMessageHistory(\n",
    "        table_name=\"SessionTable\", session_id=contactId,\n",
    "        key={\n",
    "            \"SessionId\": contactId,\n",
    "            \"type\": 'ChatHistory',\n",
    "            }\n",
    "        )\n",
    "    system_message = SystemMessage(\n",
    "        content=(system_message),\n",
    "        input_variables=['InboundMessage']\n",
    "    )\n",
    "    \n",
    "    prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "        system_message=system_message,\n",
    "        extra_prompt_messages=[\n",
    "            MessagesPlaceholder(variable_name='chat_history')\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, tools=tools, \n",
    "        verbose=verbose, return_intermediate_steps=True\n",
    "        )\n",
    "    agent_info = {\n",
    "        'agent': agent,\n",
    "        'agent_executor': agent_executor,\n",
    "        'chat_history': message_history.messages\n",
    "    }\n",
    "    return agent_info\n",
    "\n",
    "def chat_with_chatbot(user_input, agent_info):\n",
    "    start_time = time()\n",
    "    print(f'Chat history length: {len(agent_info[\"chat_history\"])}')\n",
    "    chat_history = agent_info['chat_history']\n",
    "    result = agent_info['agent_executor']({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": chat_history\n",
    "        })\n",
    "    print(f'Response time: {time() - start_time} seconds')\n",
    "    \n",
    "    return result\n",
    "\n",
    "def alert_human(str) -> str:\n",
    "    return \"[ALERT HUMAN]\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=f\"alert_human_function\",\n",
    "        func=alert_human,\n",
    "        description=f\"A function to use when you do not know how to respond to the InboundMessage because it cannot be answered from your instructions or documentation.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "conversation_id = 3\n",
    "InboundMessage = \"I have a herniated disc. Can you still coach me?\"\n",
    "reply_dict[conversation_id] = dict()\n",
    "question_id = 1\n",
    "\n",
    "contactId = contacts['me_mcloone']\n",
    "system_message_dict[conversation_id] = create_system_message(\n",
    "    'CoachMcloone', prompts_filepath='../prompts',\n",
    "    examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    ")\n",
    "conversation_dict[conversation_id] = create_chatbot(\n",
    "    contactId, system_message_dict[conversation_id], tools=tools,\n",
    "    # model='gpt-4-32k'\n",
    "    )\n",
    "\n",
    "reply_dict[conversation_id][question_id] = chat_with_chatbot(\n",
    "    InboundMessage, conversation_dict[conversation_id]\n",
    ")\n",
    "reply_text = reply_dict[conversation_id][question_id][\"output\"]\n",
    "print(f'Reply from `chat_with_chatbot`: {reply_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "{\"response\":\"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\"alert_human\":true}\n",
      "\n",
      "{\"response\":\"I'm Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\"alert_human\":true}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n{\"response\":\"I\\'m Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\"alert_human\":true}\\n\\n{\"response\":\"I\\'m Amanda - creator of the Strong and Sassy programme. I am a degree qualified nutritionist and coach with over 10 years experience. I educate women about the importance of health and wellness, so they can live a sustainable, healthy and balanced lifestyle. I help women become super confident, strong and happy within themselves.\",\"alert_human\":true}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(reply_text))\n",
    "print(reply_text)\n",
    "reply_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 4 column 1 (char 368)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparse_json_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m, in \u001b[0;36mparse_json_string\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_json_string\u001b[39m(json_string):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 4 column 1 (char 368)"
     ]
    }
   ],
   "source": [
    "parse_json_string(reply_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 4 Make sure JSON format is adhered to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 131\u001b[0m\n\u001b[1;32m    126\u001b[0m contactId \u001b[38;5;241m=\u001b[39m contacts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mme_mcloone\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    127\u001b[0m system_message_dict[conversation_id] \u001b[38;5;241m=\u001b[39m create_system_message(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoachMcloone\u001b[39m\u001b[38;5;124m'\u001b[39m, prompts_filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../prompts\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    129\u001b[0m     examples_filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/chat_examples\u001b[39m\u001b[38;5;124m'\u001b[39m, doc_filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/rag_docs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    130\u001b[0m )\n\u001b[0;32m--> 131\u001b[0m conversation_dict[conversation_id] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_chatbot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontactId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconversation_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model='gpt-4-32k'\u001b[39;49;00m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m reply_dict[conversation_id][question_id] \u001b[38;5;241m=\u001b[39m chat_with_chatbot(\n\u001b[1;32m    137\u001b[0m     InboundMessage, conversation_dict[conversation_id]\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m reply_text \u001b[38;5;241m=\u001b[39m reply_dict[conversation_id][question_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[31], line 83\u001b[0m, in \u001b[0;36mcreate_chatbot\u001b[0;34m(contactId, system_message, tools, model, verbose)\u001b[0m\n\u001b[1;32m     72\u001b[0m system_message \u001b[38;5;241m=\u001b[39m SystemMessage(\n\u001b[1;32m     73\u001b[0m     content\u001b[38;5;241m=\u001b[39m(system_message),\n\u001b[1;32m     74\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInboundMessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     76\u001b[0m parser \u001b[38;5;241m=\u001b[39m SimpleJsonOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mChatbot_Response)\n\u001b[1;32m     78\u001b[0m prompt \u001b[38;5;241m=\u001b[39m OpenAIFunctionsAgent\u001b[38;5;241m.\u001b[39mcreate_prompt(\n\u001b[1;32m     79\u001b[0m     system_message\u001b[38;5;241m=\u001b[39msystem_message,\n\u001b[1;32m     80\u001b[0m     extra_prompt_messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     81\u001b[0m         MessagesPlaceholder(variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     82\u001b[0m         ],\n\u001b[0;32m---> 83\u001b[0m     partial_variables\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_instructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_format_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     86\u001b[0m agent \u001b[38;5;241m=\u001b[39m OpenAIFunctionsAgent(llm\u001b[38;5;241m=\u001b[39mllm, tools\u001b[38;5;241m=\u001b[39mtools, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m     87\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(\n\u001b[1;32m     88\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39mtools, \n\u001b[1;32m     89\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose, return_intermediate_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     )\n",
      "File \u001b[0;32m~/repositories/GHL-chat/src/langchain_core/output_parsers/base.py:284\u001b[0m, in \u001b[0;36mBaseOutputParser.get_format_instructions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_format_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Instructions on how the LLM output should be formatted.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Chatbot_Response(BaseModel):\n",
    "    response: str = Field(description=\"The response to the InboundMessage.\")\n",
    "    alert_human: bool = Field(description=\"Whether or not to alert a human to review the response.\")\n",
    "\n",
    "def create_system_message(\n",
    "        business_name, prompts_filepath='../prompts',\n",
    "        examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    "        ):\n",
    "    \n",
    "    instructions = load_txt(f'{business_name}.md', prompts_filepath)\n",
    "    examples = load_txt(f'{business_name}.txt', examples_filepath)\n",
    "    document = load_txt(f'{business_name}_doc.md', doc_filepath)\n",
    "\n",
    "    system_message = f\"\"\"# Stage 1\n",
    "\n",
    "{instructions}\n",
    "\n",
    "Return your response on a JSON format with the following keys:\n",
    "- \"response\" (string): The response to the InboundMessage.\n",
    "- \"alert_human\" (True or False): Whether or not to alert a human to review the response.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are example conversations with leads. Each lead as a unique contact ID.\n",
    "An InboundMessage is from the lead. An OutboundMessage is from you.\n",
    "\n",
    "{examples}\n",
    "\n",
    "## Relevant documentation\n",
    "\n",
    "{document}\n",
    "\n",
    "# Stage 2\n",
    "\n",
    "Review your response from stage 1. \n",
    "Revise your response if needed to make sure you followed the instructions.\n",
    "Make sure that if the question cannot be answered through the documentation, \n",
    "you return \"[ALERT HUMAN]\".\n",
    "\n",
    "# Stage 3\n",
    "\n",
    "Review your response from stage 2 to ensure your response is concise.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Write the next OutboundMessage based on the following InboundMessage, \n",
    "    which is delimited by triple backticks: ```{InboundMessage}```\n",
    "    \"\"\"\n",
    "    system_message = f'{system_message}{prompt}'\n",
    "    return system_message\n",
    "\n",
    "def create_chatbot(contactId, system_message, tools, model=\"gpt-3.5-turbo-1106\", verbose=True):\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        temperature = 0,\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key'],\n",
    "        model=model, \n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}} # https://platform.openai.com/docs/guides/text-generation/json-mode  # https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.openai.ChatOpenAI.html?highlight=chatopenai#\n",
    "        )\n",
    "    message_history = DynamoDBChatMessageHistory(\n",
    "        table_name=\"SessionTable\", session_id=contactId,\n",
    "        key={\n",
    "            \"SessionId\": contactId,\n",
    "            \"type\": 'ChatHistory',\n",
    "            }\n",
    "        )\n",
    "    system_message = SystemMessage(\n",
    "        content=(system_message),\n",
    "        input_variables=['InboundMessage']\n",
    "    )\n",
    "    parser = SimpleJsonOutputParser(pydantic_object=Chatbot_Response)\n",
    "\n",
    "    prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "        system_message=system_message,\n",
    "        extra_prompt_messages=[\n",
    "            MessagesPlaceholder(variable_name='chat_history')\n",
    "            ],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, tools=tools, \n",
    "        verbose=verbose, return_intermediate_steps=True\n",
    "        )\n",
    "    agent_info = {\n",
    "        'agent': agent,\n",
    "        'agent_executor': agent_executor,\n",
    "        'chat_history': message_history.messages\n",
    "    }\n",
    "    return agent_info\n",
    "\n",
    "def chat_with_chatbot(user_input, agent_info):\n",
    "    start_time = time()\n",
    "    print(f'Chat history length: {len(agent_info[\"chat_history\"])}')\n",
    "    chat_history = agent_info['chat_history']\n",
    "    result = agent_info['agent_executor']({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": chat_history\n",
    "        })\n",
    "    print(f'Response time: {time() - start_time} seconds')\n",
    "    \n",
    "    return result\n",
    "\n",
    "def alert_human(str) -> str:\n",
    "    return \"[ALERT HUMAN]\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=f\"alert_human_function\",\n",
    "        func=alert_human,\n",
    "        description=f\"A function to use when you do not know how to respond to the InboundMessage because it cannot be answered from your instructions or documentation.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "conversation_id = 4\n",
    "InboundMessage = \"I have a herniated disc. Can you still coach me?\"\n",
    "reply_dict[conversation_id] = dict()\n",
    "question_id = 1\n",
    "\n",
    "contactId = contacts['me_mcloone']\n",
    "system_message_dict[conversation_id] = create_system_message(\n",
    "    'CoachMcloone', prompts_filepath='../prompts',\n",
    "    examples_filepath='../data/chat_examples', doc_filepath='../data/rag_docs'\n",
    ")\n",
    "conversation_dict[conversation_id] = create_chatbot(\n",
    "    contactId, system_message_dict[conversation_id], tools=tools,\n",
    "    # model='gpt-4-32k'\n",
    "    )\n",
    "\n",
    "reply_dict[conversation_id][question_id] = chat_with_chatbot(\n",
    "    InboundMessage, conversation_dict[conversation_id]\n",
    ")\n",
    "reply_text = reply_dict[conversation_id][question_id][\"output\"]\n",
    "print(f'Reply from `chat_with_chatbot`: {reply_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ghl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
